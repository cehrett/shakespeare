{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started\n",
    "#### To start this notebook, you must have a huggingface account and request access from Meta to use Llama 2.\n",
    "https://huggingface.co/\n",
    "\n",
    "#### In huggingface, create an access token\n",
    "https://huggingface.co/docs/hub/security-tokens\n",
    "\n",
    "#### Inside your home directory access .apikeys and create a huggingface_api_key.txt and paste you access token inside the file\n",
    "path - /home/{your username}/.apikeys\n",
    "#### Using the link below request access from Meta\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "#### Once you recieve access from Meta inside terminal create a conda environment using\n",
    "conda create --name {environment_name} python=3.10\n",
    "\n",
    "#### Then Install ipykernel using\n",
    "conda install ipykernel\n",
    "\n",
    "#### To allow your environment to be used in the notebook run the following line and select your environment on the top right besides the debugging symbol\n",
    "python -m ipykernel install --user --name={environment_name}\n",
    "\n",
    "#### Go back to terminal and install all the packages with\n",
    "pip install -r packages.txt\n",
    "\n",
    "\n",
    "#### Edit the data set, test set, and validation set under Load Datasets with the path and you are good to go!\n",
    "##### All imported data must be a csv. Csv must have at least 2 columns for output and test\n",
    "##### Import the data inside the llama-recipes folder\n",
    "##### Always run the first 3 cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Huggingface API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/sdj5/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory and load Huggingface api key\n",
    "import os\n",
    "\n",
    "username = os.getenv('USER')\n",
    "directory_path = os.path.join('/scratch', username)\n",
    "\n",
    "# Set Huggingface cache directory to be on scratch drive\n",
    "if os.path.exists(directory_path):\n",
    "    hf_cache_dir = os.path.join(directory_path, 'hf_cache')\n",
    "    if not os.path.exists(hf_cache_dir):\n",
    "        os.mkdir(hf_cache_dir)\n",
    "    print(f\"Okay, using {hf_cache_dir} for huggingface cache. Models will be stored there.\")\n",
    "    assert os.path.exists(hf_cache_dir)\n",
    "    os.environ['TRANSFORMERS_CACHE'] = f'/scratch/{username}/hf_cache/'\n",
    "else:\n",
    "    error_message = f\"Are you sure you entered your username correctly? I couldn't find a directory {directory_path}.\"\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load Huggingface api key\n",
    "api_key_loc = os.path.join('/home', username, '.apikeys', 'huggingface_api_key.txt')\n",
    "\n",
    "if os.path.exists(api_key_loc):\n",
    "    print('Huggingface API key loaded.')\n",
    "    with open(api_key_loc, 'r') as api_key_file:\n",
    "        huggingface_api_key = api_key_file.read().strip()  # Read and store the contents\n",
    "else:\n",
    "    error_message = f'Huggingface API key not found. You need to get an HF API key from the HF website and store it at {api_key_loc}.\\n' \\\n",
    "                    'The API key will let you download models from Huggingface.'\n",
    "    raise FileNotFoundError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from configs import fsdp_config, train_config\n",
    "from peft import get_peft_model, prepare_model_for_int8_training, PeftModelForCausalLM, LoraConfig, TaskType, prepare_model_for_int8_training, PeftModel\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "from utils.train_utils import (\n",
    "    train,\n",
    "    freeze_transformer_layers,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    clear_gpu_cache,\n",
    "    print_model_size,\n",
    "    #get_policies\n",
    ")\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "from configs.datasets import samsum_dataset, alpaca_dataset, grammar_dataset\n",
    "from ft_datasets.utils import Concatenator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e328100a057a49e3a5c18f9c1903ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdj5/.local/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        token=huggingface_api_key,\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        token=huggingface_api_key\n",
    ")\n",
    "#the code will output \"Error displaying widget: model not found\" it is not an error just the code failing to create a loading bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sdj5/.cache/huggingface/datasets/csv/default-5defd72c5a44f7d1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/sdj5/.cache/huggingface/datasets/csv/default-d3211e4ef040aa2b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached shuffled indices for dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-5defd72c5a44f7d1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-85573b2a742d2739.arrow\n",
      "Loading cached shuffled indices for dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-d3211e4ef040aa2b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-aae07c843831a232.arrow\n",
      "Loading cached shuffled indices for dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e648dda52a599d9b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add testset and rename current test set to validation set \n",
    "\n",
    "#edit file path to your unique dataset\n",
    "dataset = load_dataset('csv', data_files='train_set.csv',split = 'train')\n",
    "valset = load_dataset('csv', data_files='validation_set.csv',split = 'train')\n",
    "testset = load_dataset('csv', data_files='test_set.csv',split = 'train')\n",
    "\n",
    "\n",
    "#Sampling only for testing\n",
    "sample_fraction = 1\n",
    "\n",
    "num_samples = int(len(dataset) * sample_fraction)\n",
    "num_s = int(len(valset) * sample_fraction)\n",
    "num_sa = int(len(testset) * sample_fraction)\n",
    "\n",
    "# Sample 1/10 of the data randomly\n",
    "dataset = dataset.shuffle(seed=42).select(list(range(num_samples)))\n",
    "valset = valset.shuffle(seed=42).select(list(range(num_s)))\n",
    "testset = testset.shuffle(seed=42).select(list(range(num_sa)))\n",
    "\n",
    "#Edit the prompt to tell the model what to do\n",
    "prompt = (\n",
    "    f\"{{llama_prompt}}{{next_line}}{{eos_token}}\"\n",
    ")\n",
    "\n",
    "#edit the variables in prompt.format to match your data: essentially what you what the model to read\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            llama_prompt = sample[\"llama_prompt\"],\n",
    "            next_line = sample[\"next_line\"],\n",
    "            eos_token=tokenizer.eos_token,\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "valset = valset.map(apply_prompt_template, remove_columns=list(valset.features))\n",
    "testset = testset.map(apply_prompt_template, remove_columns=list(testset.features))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "valset = valset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(valset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "testset = testset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(testset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "\n",
    "\n",
    "train_dataset = dataset\n",
    "val_dataset = dataset\n",
    "test_dataset = testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached shuffled indices for dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e648dda52a599d9b.arrow\n",
      "Loading cached processed dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e0f6e0836e353a7d.arrow\n",
      "Loading cached processed dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cae86e3e8260cc69.arrow\n",
      "Loading cached processed dataset at /home/sdj5/.cache/huggingface/datasets/csv/default-9f4eae84c3258a64/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-28427d9383226983.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\": \\nQ. Dor.: The thirsty earth is broke with many a gap,\\n \\tAnd lands are lean where rivers do not run:\\n\\tWhere soul is reft from that it loveth best,\\n\\n\\nNext line:\\n[/INST]How can it thrive or boast of quiet rest?</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: The Scottish History of James the Fourth\\nYear: 1590\\nAuthor: Robert Greene\\nGenre: Romance\\nCharacters in play: ['Q. Dor.', '1st Lady.', 'Lawyer.', '1st Revel.', 'Purv.', 'Bart.', 'K. of Eng.', 'Ross.', 'Scout.', 'Andrew.', 'Morton.', 'Slip.', 'All.', 'K. James.', 'Bohan.', 'Ida.', '1st Hunts.', 'Lady A.', 'Bishop.', 'Ober.', 'Jaques.', 'Doug.', 'Ateuk.', 'Merch.', 'Cuth.', 'Nano.', 'Divine.', 'C. of Arran.', 'Eust.']\\nLines of play: \\nK. James.: Andrew?\\n Andrew.: Ay, my liege.\\n K. James.: What news?\\n \\n\\nNext line:\\n[/INST]Andrew.: I think my mouth was made at first</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: A Looking Glass for London\\nYear: 1590\\nAuthor: Robert Greene and Thomas Lodge\\nGenre: Morality\\nCharacters in play: ['Adam.', '2nd Search.', '1st Lady.', 'K. of Crete.', 'Alvida.', '1st Merch.', 'nan', 'Oseas.', 'K. of Cilicia.', 'Jonas.', '2nd Ruf.', 'Devil.', 'Angel.', '1st Magus.', 'Clesiph.', 'Gov.', '1st Lord.', 'S’s Wife.', '1st Search.', '2nd. Ruffian.', 'Smith.', 'K. of Paph.', '1st Sailor.', 'Samia.', 'Lawyer.', 'Thrasy.', 'Usurer.', 'Rasni.', 'Master.', 'Remil.', '1st Ruf.', 'Judge.', 'Alcon.', 'Radag.']\\nLines of play: \\nOseas.: And charity exíled from rich men's door;\\n \\tWhen men by wit do labour to disprove\\n\\tThe plagues for sin sent down by God above;\\n\\n\\nNext line:\\n[/INST]Where great men's ears are stopped to good advice,</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: The Scottish History of James the Fourth\\nYear: 1590\\nAuthor: Robert Greene\\nGenre: Romance\\nCharacters in play: ['Ross.', 'Eust.', 'Bishop.', 'Cuth.', '1st Hunts.', 'Q. Dor.', '1st Revel.', 'Morton.', 'Merch.', 'Bart.', 'Ateuk.', 'Doug.', 'Divine.', 'Lawyer.', '1st Lady.', 'Ober.', 'Scout.', 'Jaques.', 'K. of Eng.', 'C. of Arran.', 'Ida.', 'Bohan.', 'K. James.', 'All.', 'Nano.', 'Purv.', 'Slip.', 'Lady A.', 'Andrew.']\\nLines of play: \\nSlip.: First, a merry countenance;\\n \\tSecond, a soft pace;\\n\\tThird, a broad forehead;\\n\\n\\nNext line:\\n[/INST]Fourth, broad buttocks;</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: The Scottish History of James the Fourth\\nYear: 1590\\nAuthor: Robert Greene\\nGenre: Romance\\nCharacters in play: ['1st Hunts.', 'Nano.', 'Bohan.', 'Bishop.', 'All.', '1st Lady.', 'Doug.', 'Merch.', 'Jaques.', 'Ateuk.', 'Lawyer.', 'Purv.', 'Ober.', 'Cuth.', 'K. James.', 'Bart.', 'Lady A.', 'Q. Dor.', 'Scout.', 'Andrew.', 'Morton.', 'Ida.', 'Divine.', 'C. of Arran.', 'Ross.', 'Slip.', 'Eust.', '1st Revel.', 'K. of Eng.']\\nLines of play: \\nAteuk.: She will, my lord.\\n K. James.: Then let her die: devise, advise the means;\\n \\tAll likes me well that lends me hope in love.\\n\\n\\nNext line:\\n[/INST]Ateuk.: What, will your grace consent? Then let me work.</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: A Looking Glass for London\\nYear: 1590\\nAuthor: Robert Greene and Thomas Lodge\\nGenre: Morality\\nCharacters in play: ['2nd Ruf.', 'K. of Crete.', 'Oseas.', '1st Lord.', 'Devil.', 'Judge.', 'Radag.', 'Thrasy.', 'Samia.', 'Jonas.', '1st Lady.', '1st Search.', 'nan', 'K. of Paph.', '1st Sailor.', '2nd Search.', 'Rasni.', '1st Magus.', 'K. of Cilicia.', '1st Merch.', 'Lawyer.', 'Alvida.', 'Adam.', 'Gov.', '2nd. Ruffian.', 'S’s Wife.', 'Master.', 'Usurer.', 'Alcon.', 'Clesiph.', 'Angel.', 'Smith.', '1st Ruf.', 'Remil.']\\nLines of play: \\nS’s Wife.: never proffered you any wrong.\\n Smith.: Nay, whore, thy part shall not be behind.\\n Adam.: Why, suppose, master, I have offended you,\\n \\n\\nNext line:\\n[/INST]is it lawful for the master to beat the servant for all</s><s> [INST]<<SYS>>\\nYou are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\\n<</SYS>>\\n\\nTitle: A Looking Glass for London\\nYear: 1590\\nAuthor: Robert Greene and Thomas Lodge\\nGenre: Morality\\nCharacters in play: ['Rasni.', 'Master.', 'K. of Crete.', 'Oseas.', 'Judge.', 'Devil.', 'K. of Paph.', '1st Ruf.', 'Radag.', 'S’s Wife.',\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = load_dataset('csv', data_files='test_set.csv',split = 'train')\n",
    "sample_fraction = .3\n",
    "num_sa = int(len(testset) * sample_fraction)\n",
    "testset = testset.shuffle(seed=42).select(list(range(num_sa)))\n",
    "testset = testset.map(apply_prompt_template, remove_columns=list(testset.features))\n",
    "testset = testset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(testset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "tokenizer.decode(testset[\"labels\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
      "\n",
      "\n",
      "Title: Dido, Queen of Carthage\n",
      "Year: 1585-1586\n",
      "Author: Christopher Marlowe\n",
      "Genre: Tragedy\n",
      "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
      "Lines of play: \n",
      "Jup.: Come, gentle Ganymede, and play with me:\n",
      " \tI love thee well, say Juno what she will.\n",
      "Gany.: I am much better for your worthless love,\n",
      " \n",
      "\n",
      "Next line:\n",
      "\"\n",
      "\n",
      "### Examples\n",
      "\n",
      "```\n",
      "Aeneas. \tAeneas.\n",
      "Juno. \tJuno.\n",
      "Dido. \tDido.\n",
      "Cloan. \tCloan.\n",
      "Gany. \tGany.\n",
      "Jup. \tJup.\n",
      "Anna. \tAnna.\n",
      "```\n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "import re\n",
      "import collections\n",
      "\n",
      "\n",
      "class Solution:\n",
      "    def\n"
     ]
    }
   ],
   "source": [
    "#Edit eval_prompt to match your data\n",
    "eval_prompt = \"\"\"\n",
    "\"You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
    "\n",
    "\n",
    "Title: Dido, Queen of Carthage\n",
    "Year: 1585-1586\n",
    "Author: Christopher Marlowe\n",
    "Genre: Tragedy\n",
    "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
    "Lines of play: \n",
    "Jup.: Come, gentle Ganymede, and play with me:\n",
    " \tI love thee well, say Juno what she will.\n",
    "Gany.: I am much better for your worthless love,\n",
    " \n",
    "\n",
    "Next line:\n",
    "\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]<<SYS>>\n",
      "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
      "<</SYS>>\n",
      "\n",
      "Title: Dido, Queen of Carthage\n",
      "Year: 1585-1586\n",
      "Author: Christopher Marlowe\n",
      "Genre: Tragedy\n",
      "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
      "Lines of play: \n",
      "Jup.: Come, gentle Ganymede, and play with me:\n",
      " \tI love thee well, say Juno what she will.\n",
      "Gany.: I am much better for your worthless love,\n",
      " \n",
      "\n",
      "Next line:\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Edit eval_prompt to match your data\n",
    "eval_prompt = \"\"\"\n",
    "[INST]<<SYS>>\n",
    "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
    "<</SYS>>\n",
    "\n",
    "Title: Dido, Queen of Carthage\n",
    "Year: 1585-1586\n",
    "Author: Christopher Marlowe\n",
    "Genre: Tragedy\n",
    "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
    "Lines of play: \n",
    "Jup.: Come, gentle Ganymede, and play with me:\n",
    " \tI love thee well, say Juno what she will.\n",
    "Gany.: I am much better for your worthless love,\n",
    " \n",
    "\n",
    "Next line:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enables Parameter Efficient Finetuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdj5/.local/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "#reduces the parameters needed to train\n",
    "model.train()\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias= \"none\",\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    kwargs = {\n",
    "        'use_peft': True, \n",
    "        'peft_method': 'lora', \n",
    "        'quantization': True, \n",
    "        'use_fp16': True, \n",
    "        'model_name': os.path.join('/scratch', username, 'models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9'), \n",
    "        'output_dir': os.path.join('/scratch', username)\n",
    "    }\n",
    "    \n",
    "    update_config((train_config, fsdp_config), **kwargs)\n",
    "    \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    peft_config = generate_peft_config(train_config, kwargs)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "#set up the configurations for training\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines training arguments and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/sdj5/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sdj5/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='1293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  21/1293 03:25 < 3:49:26, 0.09 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='550' max='647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [550/647 30:39 < 05:25, 0.30 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True, \n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=20,  # 10\n",
    "    save_strategy=\"steps\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    auto_find_batch_size = True, \n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'},\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=20,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "    \n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dirrectory of working model\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "\n",
    "#saves the model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "#Saves model configurations\n",
    "model.config.to_json_file(os.path.join(output_dir, \"config.json\"))\n",
    "\n",
    "#saves PEFT config\n",
    "peft_config = model.peft_config\n",
    "json_file_path = os.path.join(output_dir, \"peft_config.json\")\n",
    "\n",
    "# Custom serialization function for LoraConfig objects\n",
    "def lora_config_serializer(obj):\n",
    "    if isinstance(obj, LoraConfig):\n",
    "        # Return a dictionary representation of the LoraConfig object\n",
    "        return obj.__dict__\n",
    "    raise TypeError(\"Type not serializable\")\n",
    "\n",
    "# Write the dictionary to the JSON file using the custom serializer\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(peft_config, json_file, default=lora_config_serializer, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on the same input as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"[INST]<<SYS>>\n",
      "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
      "<</SYS>>\n",
      "\n",
      "Title: Dido, Queen of Carthage\n",
      "Year: 1585-1586\n",
      "Author: Christopher Marlowe\n",
      "Genre: Tragedy\n",
      "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
      "Lines of play: \n",
      "Jup.: Come, gentle Ganymede, and play with me:\n",
      " \tI love thee well, say Juno what she will.\n",
      "Gany.: I am much better for your worthless love,\n",
      " \n",
      "\n",
      "Next line:\n",
      "[/INST]\"\n",
      "\n",
      "INST>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tests the model on the sample input from before\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81664e2ffaaf4b24a2d79fa7a020775a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]<<SYS>>\n",
      "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
      "<</SYS>>\n",
      "\n",
      "Title: Dido, Queen of Carthage\n",
      "Year: 1585-1586\n",
      "Author: Christopher Marlowe\n",
      "Genre: Tragedy\n",
      "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
      "Lines of play: \n",
      "Jup.: Come, gentle Ganymede, and play with me:\n",
      " \tI love thee well, say Juno what she will.\n",
      "Gany.: I am much better for your worthless love,\n",
      " \n",
      "\n",
      "Next line:\n",
      "[/INST]\n",
      "Next line:\n",
      "[INST]\n",
      "You are the only one I do not despise.\n"
     ]
    }
   ],
   "source": [
    "#load and test- get metrics and automatically show random 5 model input and output\n",
    "#get this cell running on cold\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        train_config.model_name,\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    ")    \n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(model, output_dir)\n",
    "\n",
    "# Assuming test_dataset is your test dataset and model is your trained model\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "'''\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Assuming references is a list of reference texts and hypotheses is a list of generated texts\n",
    "def compute_bleu(references, hypotheses):\n",
    "    # Compute BLEU score using nltk's corpus_bleu function\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "# Define data collator for evaluation\n",
    "eval_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create Trainer instance for evaluation\n",
    "eval_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_eval_batch_size=8,  # Adjust batch size based on your system's memory\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_training_args,\n",
    "    data_collator=eval_data_collator,\n",
    "    compute_metrics=lambda p: {\"bleu\": compute_bleu(p)},\n",
    ")\n",
    "\n",
    "# Evaluate the model on test_dataset\n",
    "evaluation_results = eval_trainer.predict(test_dataset)\n",
    "'''\n",
    "eval_prompt = \"\"\"\n",
    "[INST]<<SYS>>\n",
    "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
    "<</SYS>>\n",
    "\n",
    "Title: Dido, Queen of Carthage\n",
    "Year: 1585-1586\n",
    "Author: Christopher Marlowe\n",
    "Genre: Tragedy\n",
    "Characters in play: ['Aeneas.', 'First Lord.', 'Cloan.', 'Dido.', 'Achat.', 'Nurse.', 'Serg.', 'Jup.', 'Ilio.', 'Venus.', 'Iarb.', 'nan', 'Asca.', 'Cupid.', 'Juno.', 'Anna.']\n",
    "Lines of play: \n",
    "Jup.: Come, gentle Ganymede, and play with me:\n",
    " \tI love thee well, say Juno what she will.\n",
    "Gany.: I am much better for your worthless love,\n",
    " \n",
    "\n",
    "Next line:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]<<SYS>>\n",
      "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
      "<</SYS>>\n",
      "\n",
      "Title: Love's Metamorphosis\n",
      "Year: 1590\n",
      "Author: John Lyly\n",
      "Genre: Mythological\n",
      "Characters in play: ['nan', 'Protea.', 'Silv.', 'Celia.', 'Fidelia.', 'Ceres.', 'Petul.', 'Mont.', 'Tirt.', 'Eris.', 'Ramis.', 'Siren.', 'Cupid.', 'Nisa.', 'Merch.', 'Niobe.']\n",
      "Lines of play: \n",
      "Ramis.: I cannot see, Montanus, why it is fained by\n",
      " \tthe poets that Love sat upon the chaos and created\n",
      "\tthe world, since in the world there is so little love.\n",
      "\n",
      "\n",
      "Next line:\n",
      "[/INST]Next line: \t\n",
      "Cupid.: Nay, if you will not love, you shall not live.\n"
     ]
    }
   ],
   "source": [
    "#Edit eval_prompt to match your data\n",
    "eval_prompt = \"\"\"\n",
    "[INST]<<SYS>>\n",
    "You are an expert at predicting the next line of Elizabethan plays. Given the title of a play, its published year, author, genre, characters in the play, and three lines, generate the next line that follows. Only generate the next line with no other introduction or explanation.\n",
    "<</SYS>>\n",
    "\n",
    "Title: Love's Metamorphosis\n",
    "Year: 1590\n",
    "Author: John Lyly\n",
    "Genre: Mythological\n",
    "Characters in play: ['nan', 'Protea.', 'Silv.', 'Celia.', 'Fidelia.', 'Ceres.', 'Petul.', 'Mont.', 'Tirt.', 'Eris.', 'Ramis.', 'Siren.', 'Cupid.', 'Nisa.', 'Merch.', 'Niobe.']\n",
    "Lines of play: \n",
    "Ramis.: I cannot see, Montanus, why it is fained by\n",
    " \tthe poets that Love sat upon the chaos and created\n",
    "\tthe world, since in the world there is so little love.\n",
    "\n",
    "\n",
    "Next line:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'llama_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate and display summaries\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(random_sample_indices, random_samples):\n\u001b[0;32m---> 10\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[43mapply_prompt_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m     generated_summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(evaluation_results\u001b[38;5;241m.\u001b[39mpredictions[idx], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mapply_prompt_template\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_prompt_template\u001b[39m(sample):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 30\u001b[0m             llama_prompt \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     31\u001b[0m             next_line \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_line\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m             eos_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token,\n\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m     }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'llama_prompt'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get random samples for display\n",
    "num_samples_to_display = 5\n",
    "random_sample_indices = random.sample(range(len(test_dataset)), num_samples_to_display)\n",
    "random_samples = [test_dataset[idx] for idx in random_sample_indices]\n",
    "\n",
    "# Generate and display summaries\n",
    "for idx, sample in zip(random_sample_indices, random_samples):\n",
    "    input_text = apply_prompt_template(sample)[\"text\"]\n",
    "    generated_summary = tokenizer.decode(evaluation_results.predictions[idx], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Sample {idx + 1}:\")\n",
    "    print(f\"Input Dialogue:\\n{sample['llama_prompt']}\")\n",
    "    print(f\"Generated Summary:\\n{generated_summary}\\n{'-'*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
